{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00c9ccd8-a07d-4feb-957d-5c321e4ac8be",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0742437-fcdd-4527-baa0-dd20b9dbb48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback, CallbackList\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.simplefilter(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8ebef1-4931-4b2c-b6aa-097582c79a43",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4653ce45-945e-49b5-a330-3b1166e8b855",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "easy = [[1, 1, 1, 1, 1],\n",
    "        [1, 0, 0, 0, 1],\n",
    "        [1, 1, 1, 1, 1]]\n",
    "\n",
    "medium = [[1, 1, 1, 1, 1, 1, 1, 1],\n",
    "          [1, 0, 0, 1, 1, 0, 0, 1],\n",
    "          [1, 0, 0, 1, 0, 0, 0, 1],\n",
    "          [1, 1, 0, 0, 0, 1, 1, 1],\n",
    "          [1, 0, 0, 1, 0, 0, 0, 1],\n",
    "          [1, 0, 1, 0, 0, 1, 0, 1],\n",
    "          [1, 0, 0, 0, 1, 0, 0, 1],\n",
    "          [1, 1, 1, 1, 1, 1, 1, 1]]\n",
    "\n",
    "hard = [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n",
    "        [1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1],\n",
    "        [1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1],\n",
    "        [1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1],\n",
    "        [1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
    "\n",
    "maps = {'easy': easy, 'medium': medium, 'hard': hard}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a0380326-cfd0-41bf-b260-6bb40bcf6491",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridMazeEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(self, maps):\n",
    "        super().__init__()\n",
    "        self.maps = maps\n",
    "        self.load_map(\"hard\")\n",
    "\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=max(self.width, self.height), shape=(2,), dtype=np.float32)\n",
    "        self.action_space = gym.spaces.Box(low=-1, high=1, shape=(2,), dtype=np.float32)\n",
    "\n",
    "        free_cells = np.argwhere(self.grid == 0)\n",
    "        self.agent_pos = free_cells[0] + 0.5\n",
    "\n",
    "    def load_map(self, map_name):\n",
    "        self.map_name = map_name\n",
    "        self.grid = np.array(self.maps[map_name])\n",
    "        self.height, self.width = self.grid.shape\n",
    "\n",
    "        free_cells = np.argwhere(self.grid == 0)\n",
    "        self.agent_pos = free_cells[0] + 0.5\n",
    "        self.goal_pos = free_cells[-1] + 0.5\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        free_cells = np.argwhere(self.grid == 0)\n",
    "        self.agent_pos = free_cells[0] + 0.5\n",
    "        return self.agent_pos.copy(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        delta = np.clip(action, -1, 1) * 0.3\n",
    "        new_pos = self.agent_pos + delta\n",
    "\n",
    "        if not (0 <= new_pos[0] < self.width and 0 <= new_pos[1] < self.height):\n",
    "            new_pos = self.agent_pos\n",
    "\n",
    "        cell_x, cell_y = int(new_pos[0]), int(new_pos[1])\n",
    "        if self.grid[cell_y, cell_x] == 1:\n",
    "            new_pos = self.agent_pos\n",
    "\n",
    "        self.agent_pos = new_pos\n",
    "        dist_to_goal = np.linalg.norm(self.agent_pos - self.goal_pos)\n",
    "        terminated = dist_to_goal < 0.5\n",
    "        truncated = False\n",
    "        reward = -dist_to_goal\n",
    "        info = {}\n",
    "\n",
    "        return self.agent_pos.copy(), reward, terminated, truncated, info\n",
    "\n",
    "    def render(self):\n",
    "        # Simple console render:\n",
    "        print(f\"\\nMap: {self.map_name}\")\n",
    "        for y in range(self.height):\n",
    "            row = \"\"\n",
    "            for x in range(self.width):\n",
    "                if int(self.agent_pos[0]) == x and int(self.agent_pos[1]) == y:\n",
    "                    row += \"A\"  # Agent\n",
    "                elif int(self.goal_pos[0]) == x and int(self.goal_pos[1]) == y:\n",
    "                    row += \"G\"  # Goal\n",
    "                elif self.grid[y, x] == 1:\n",
    "                    row += \"#\"  # Wall\n",
    "                else:\n",
    "                    row += \".\"  # Free space\n",
    "            print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "401e190e-3743-4355-86f3-ba62fb22250b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StructuredMapSwitchCallback(BaseCallback):\n",
    "    def __init__(self, env, total_timesteps, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_timesteps = total_timesteps\n",
    "        self.milestones = [total_timesteps // 3, 2 * total_timesteps // 3]\n",
    "        self.current_map = None\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        num_steps = self.num_timesteps\n",
    "\n",
    "        if num_steps < self.milestones[0]:\n",
    "            desired_map = \"easy\"\n",
    "            difficulty = 0\n",
    "        elif num_steps < self.milestones[1]:\n",
    "            desired_map = \"medium\"\n",
    "            difficulty = 1\n",
    "        else:\n",
    "            desired_map = \"hard\"\n",
    "            difficulty = 2\n",
    "\n",
    "        if desired_map != self.current_map:\n",
    "            self.env.load_map(desired_map)\n",
    "            self.current_map = desired_map\n",
    "            if self.verbose > 0:\n",
    "                print(f\"Step {num_steps}: Switched to map '{desired_map}'\")\n",
    "\n",
    "        self.logger.record(\"map/difficulty\", difficulty)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "46e42ae7-9b26-473a-bb58-6cbc2bfbaccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsoleRenderCallback(BaseCallback):\n",
    "    def __init__(self, env, render_freq=1000, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.env = env\n",
    "        self.render_freq = render_freq\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.render_freq == 0:\n",
    "            self.env.render()\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f2aba3-21b2-4187-89cb-4bde78f1eb04",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a742e545-8658-4472-b90a-b6edfe4da7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_timesteps = 10240000\n",
    "env = GridMazeEnv(maps)\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"Training/logs/\")\n",
    "\n",
    "map_switch_callback = StructuredMapSwitchCallback(env, total_timesteps=total_timesteps, verbose=1)\n",
    "render_callback = ConsoleRenderCallback(env, render_freq=1000, verbose=0)\n",
    "callbacks = CallbackList([map_switch_callback, render_callback])\n",
    "\n",
    "model.learn(total_timesteps=total_timesteps, callback=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4b5895-392d-4ec2-8a20-626d0555e7f8",
   "metadata": {},
   "source": [
    "Use \"tensorboard --logdir logs/\" in the Training terminal to get to tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f6406d-5625-4f63-abea-f90a69b40f8f",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3ec7ab9a-71cb-4641-8daf-a36280c5b978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "env = GridMazeEnv(maps)\n",
    "model1 = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"Training/logs/PPO_1\")\n",
    "model2 = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"Training/logs/PPO_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab04238-3462-46e4-a9b7-2363e71592b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "for i in range(1, 2):\n",
    "    if i == 1:\n",
    "        evaluate_policy(model1, env, n_eval_episodes=10, render=True)\n",
    "        print(\"\\n\")\n",
    "        print(\"=\"*50)\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        evaluate_policy(model2, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6105a6e3-d5f7-426b-86ef-fd98eb04cd08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "episodes = 5\n",
    "for i in range(1, episodes+1):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model1.predict(obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        score += reward        \n",
    "    \n",
    "    print(f\"Episode: {i} Score: {score}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6026ef-eecc-4be6-9db5-3581bff3d385",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 5\n",
    "for i in range(1, episodes+1):\n",
    "    obs, _ = env.reset()\n",
    "    truncated = False\n",
    "    terminated = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model2.predict(obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        score += reward        \n",
    "    \n",
    "    print(f\"Episode: {i} Score: {score}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6317d38b-60c5-4be1-916d-e7462311c063",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3] *",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00c9ccd8-a07d-4feb-957d-5c321e4ac8be",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0742437-fcdd-4527-baa0-dd20b9dbb48b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback, CallbackList\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.simplefilter(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8ebef1-4931-4b2c-b6aa-097582c79a43",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4653ce45-945e-49b5-a330-3b1166e8b855",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "easy = [[1, 1, 1, 1, 1],\n",
    "        [1, 0, 0, 0, 1],\n",
    "        [1, 1, 1, 1, 1]]\n",
    "\n",
    "medium = [[1, 1, 1, 1, 1, 1, 1, 1],\n",
    "          [1, 0, 0, 1, 1, 0, 0, 1],\n",
    "          [1, 0, 0, 1, 0, 0, 0, 1],\n",
    "          [1, 1, 0, 0, 0, 1, 1, 1],\n",
    "          [1, 0, 0, 1, 0, 0, 0, 1],\n",
    "          [1, 0, 1, 0, 0, 1, 0, 1],\n",
    "          [1, 0, 0, 0, 1, 0, 0, 1],\n",
    "          [1, 1, 1, 1, 1, 1, 1, 1]]\n",
    "\n",
    "hard = [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n",
    "        [1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1],\n",
    "        [1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1],\n",
    "        [1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1],\n",
    "        [1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
    "\n",
    "maps = {'easy': easy, 'medium': medium, 'hard': hard}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a0380326-cfd0-41bf-b260-6bb40bcf6491",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridMazeEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(self, maps, render_mode=None):\n",
    "        super().__init__()\n",
    "        self.maps = maps\n",
    "        self.render_mode = render_mode\n",
    "        self.load_map(\"hard\")\n",
    "\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=max(self.width, self.height),\n",
    "            shape=(2,), dtype=np.float32\n",
    "        )\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "\n",
    "        self.agent_pos = None\n",
    "\n",
    "    def load_map(self, map_name):\n",
    "        self.map_name = map_name\n",
    "        self.grid = np.array(self.maps[map_name])\n",
    "        self.height, self.width = self.grid.shape\n",
    "\n",
    "        free_cells = np.argwhere(self.grid == 0)\n",
    "        self.agent_pos = free_cells[0] + 0.5\n",
    "        self.goal_pos = free_cells[-1] + 0.5\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        free_cells = np.argwhere(self.grid == 0)\n",
    "        self.agent_pos = free_cells[0] + 0.5\n",
    "        return self.agent_pos.copy().astype(np.float32), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        # Define movement directions for 4 discrete actions\n",
    "        move_map = {\n",
    "            0: np.array([0, -1]),  # Up\n",
    "            1: np.array([1, 0]),   # Right\n",
    "            2: np.array([0, 1]),   # Down\n",
    "            3: np.array([-1, 0])   # Left\n",
    "        }\n",
    "\n",
    "        delta = move_map[action] * 0.3\n",
    "        new_pos = self.agent_pos + delta\n",
    "\n",
    "        # Keep within bounds\n",
    "        if not (0 <= new_pos[0] < self.width and 0 <= new_pos[1] < self.height):\n",
    "            new_pos = self.agent_pos\n",
    "\n",
    "        # Block movement into walls\n",
    "        cell_x, cell_y = int(new_pos[0]), int(new_pos[1])\n",
    "        if self.grid[cell_y, cell_x] == 1:\n",
    "            new_pos = self.agent_pos\n",
    "\n",
    "        self.agent_pos = new_pos\n",
    "\n",
    "        dist_to_goal = np.linalg.norm(self.agent_pos - self.goal_pos)\n",
    "        terminated = dist_to_goal < 0.5\n",
    "        truncated = False\n",
    "        reward = -dist_to_goal\n",
    "        info = {}\n",
    "\n",
    "        return self.agent_pos.copy().astype(np.float32), reward, terminated, truncated, info\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode != \"human\":\n",
    "            return\n",
    "\n",
    "        print(f\"\\nMap: {self.map_name}\")\n",
    "        for y in range(self.height):\n",
    "            row = \"\"\n",
    "            for x in range(self.width):\n",
    "                if int(self.agent_pos[0]) == x and int(self.agent_pos[1]) == y:\n",
    "                    row += \"A\"\n",
    "                elif int(self.goal_pos[0]) == x and int(self.goal_pos[1]) == y:\n",
    "                    row += \"G\"\n",
    "                elif self.grid[y, x] == 1:\n",
    "                    row += \"#\"\n",
    "                else:\n",
    "                    row += \".\"\n",
    "            print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "401e190e-3743-4355-86f3-ba62fb22250b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StructuredMapSwitchCallback(BaseCallback):\n",
    "    def __init__(self, env, total_timesteps, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_timesteps = total_timesteps\n",
    "        self.milestones = [total_timesteps // 3, 2 * total_timesteps // 3]\n",
    "        self.current_map = None\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        num_steps = self.num_timesteps\n",
    "\n",
    "        if num_steps < self.milestones[0]:\n",
    "            desired_map = \"easy\"\n",
    "            difficulty = 0\n",
    "        elif num_steps < self.milestones[1]:\n",
    "            desired_map = \"medium\"\n",
    "            difficulty = 1\n",
    "        else:\n",
    "            desired_map = \"hard\"\n",
    "            difficulty = 2\n",
    "\n",
    "        if desired_map != self.current_map:\n",
    "            self.env.load_map(desired_map)\n",
    "            self.current_map = desired_map\n",
    "            if self.verbose > 0:\n",
    "                print(f\"Step {num_steps}: Switched to map '{desired_map}'\")\n",
    "\n",
    "        self.logger.record(\"map/difficulty\", difficulty)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "46e42ae7-9b26-473a-bb58-6cbc2bfbaccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsoleRenderCallback(BaseCallback):\n",
    "    def __init__(self, env, render_freq=1000, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.env = env\n",
    "        self.render_freq = render_freq\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.render_freq == 0:\n",
    "            self.env.render()\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f2aba3-21b2-4187-89cb-4bde78f1eb04",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a742e545-8658-4472-b90a-b6edfe4da7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_timesteps = 10240000\n",
    "env = GridMazeEnv(maps)\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"Training/logs/\")\n",
    "\n",
    "map_switch_callback = StructuredMapSwitchCallback(env, total_timesteps=total_timesteps, verbose=1)\n",
    "render_callback = ConsoleRenderCallback(env, render_freq=1000, verbose=0)\n",
    "callbacks = CallbackList([map_switch_callback, render_callback])\n",
    "\n",
    "model.learn(total_timesteps=total_timesteps, callback=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4b5895-392d-4ec2-8a20-626d0555e7f8",
   "metadata": {},
   "source": [
    "Use \"tensorboard --logdir logs/\" in the Training terminal to get to tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f6406d-5625-4f63-abea-f90a69b40f8f",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec7ab9a-71cb-4641-8daf-a36280c5b978",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridMazeEnv(maps)\n",
    "model1 = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"Training/logs/PPO_1\")\n",
    "model2 = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"Training/logs/PPO_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f94a68ef-0b08-413f-bf03-b5626233579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_path1 = os.path.join(\"Training\", \"Saved Models\", \"PPO_1\")\n",
    "model1.save(ppo_path1)\n",
    "ppo_path2 = os.path.join(\"Training\", \"Saved Models\", \"PPO_2\")\n",
    "model2.save(ppo_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0c9532e0-2c27-41d6-b12c-fb60992a9350",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = PPO.load(ppo_path1)\n",
    "model2 = PPO.load(ppo_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6105a6e3-d5f7-426b-86ef-fd98eb04cd08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "episodes = 10000\n",
    "max_steps = 10000\n",
    "\n",
    "episode_rewards_model1 = []\n",
    "episode_rewards_model2 = []\n",
    "\n",
    "for ep in range(1, episodes + 1):\n",
    "    for model, env, model_name, ep_reward_store in [\n",
    "        (model1, env1, \"Model1\", episode_rewards_model1),\n",
    "        (model2, env2, \"Model2\", episode_rewards_model2)\n",
    "    ]:\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        step = 0\n",
    "\n",
    "        while not done and step < max_steps:\n",
    "            action, _ = model.predict(obs)\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "\n",
    "            total_reward += reward[0]\n",
    "            step += 1\n",
    "\n",
    "        print(f\"[{model_name}] Episode {ep}, Total Reward: {total_reward}, Steps: {step}\")\n",
    "        ep_reward_store.append(total_reward)\n",
    "\n",
    "model1_rewards = np.array(episode_rewards_model1)\n",
    "model2_rewards = np.array(episode_rewards_model2)\n",
    "episodes_range = np.arange(1, episodes + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c8794b-41ae-42cd-8337-c85993c1620e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "def smooth(x, window=10):\n",
    "    return np.convolve(x, np.ones(window)/window, mode='valid')\n",
    "\n",
    "# Plot raw reward curves with transparency\n",
    "plt.plot(episodes_range, model1_rewards, label=\"Model1\", alpha=0.3, color='blue')\n",
    "plt.plot(episodes_range, model2_rewards, label=\"Model2\", alpha=0.3, color='red')\n",
    "\n",
    "# Plot smoothed (rolling average) reward curves\n",
    "smoothed_model1 = smooth(model1_rewards)\n",
    "smoothed_model2 = smooth(model2_rewards)\n",
    "plt.plot(episodes_range[9:], smoothed_model1, label=\"Model1 Rolling Avg\", color='blue')\n",
    "plt.plot(episodes_range[9:], smoothed_model2, label=\"Model2 Rolling Avg\", color='red')\n",
    "\n",
    "# Rolling standard deviation\n",
    "def rolling_std(x, window=10):\n",
    "    return np.array([np.std(x[max(0, i - window + 1):i + 1]) for i in range(len(x))])\n",
    "\n",
    "std1 = rolling_std(model1_rewards)\n",
    "std2 = rolling_std(model2_rewards)\n",
    "\n",
    "# Fill between ± std deviation\n",
    "plt.fill_between(episodes_range[9:], smoothed_model1 - std1[9:], smoothed_model1 + std1[9:], alpha=0.2, color='blue')\n",
    "plt.fill_between(episodes_range[9:], smoothed_model2 - std2[9:], smoothed_model2 + std2[9:], alpha=0.2, color='red')\n",
    "\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"Total Reward per Episode with Rolling Average and Std\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Boxplot of reward distributions\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.boxplot([model1_rewards, model2_rewards], labels=[\"Model1\", \"Model2\"])\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"Reward Distribution per Model\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Histogram / Distribution plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(model1_rewards, bins=30, alpha=0.5, label=\"Model1\", color='blue', density=True)\n",
    "plt.hist(model2_rewards, bins=30, alpha=0.5, label=\"Model2\", color='red', density=True)\n",
    "plt.xlabel(\"Total Reward\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Reward Distribution Histogram\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b446079-3811-43c1-b474-9ce5a0c030ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "t_stat, p_val = ttest_ind(model1_rewards, model2_rewards)\n",
    "print(f\"T-test: t={t_stat:.3f}, p={p_val:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c48b59a-a6ea-4b7d-b7e9-41e28d14ca2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"Reward\": np.concatenate([model1_rewards, model2_rewards]),\n",
    "    \"Model\": [\"Model1\"] * len(model1_rewards) + [\"Model2\"] * len(model2_rewards)\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.violinplot(x=\"Model\", y=\"Reward\", data=df, palette=[\"blue\", \"red\"], inner=\"quartile\", legend=False)\n",
    "\n",
    "plt.title(\"Reward Distribution per Model (Violin Plot)\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.grid(True, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4495818-cca9-471d-a615-87305146e59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(x=\"Model\", y=\"Reward\", data=df, palette=[\"blue\", \"red\"], inner=\"quartile\")\n",
    "sns.stripplot(x=\"Model\", y=\"Reward\", data=df, color='black', alpha=0.3, jitter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4ba000-8624-44a0-8a0b-bf42ff55cd53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

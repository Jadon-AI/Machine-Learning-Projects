{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "60909545-c6ab-4222-8f3d-40ed17f5e19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import NFKC, NFKD, Nmt\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "7147a516-de74-4db1-b411-c6973d9890a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "4eb40f32-cbd6-4033-924b-281723f2ccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    for i in range(0, len(df), 1000):\n",
    "        yield df[i:i+1000][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "b9170e97-cecf-49fd-97ec-89591cc04e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dealing with unknown words\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "8b298e02-63fe-4df3-9a70-8ac83c2d70bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize\n",
    "tokenizer.normalizer = normalizers.Sequence([\n",
    "    NFKC(),\n",
    "    NFKD(),\n",
    "    Nmt()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "d9d65290-08b0-47ea-b78d-e168328febf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splt words based on white space\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "59793055-281f-4537-8e7f-62847c61b6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = BpeTrainer(vocab_size=1000000, special_tokens=special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "34d74a46-1eab-4021-81ca-3b69c3493160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "7ff36e8c-5f8d-4088-add9-c6f6164049fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "fc65699d-8da2-4620-8031-9773d6d94614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['[CLS]', 'Hello', 'World', '!', 'My', 'name', 'is', 'Jay', ',', 'what', \"'\", 's', 'y', 'ours', '?', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"Hello World! My name is Jay, what's yours?\")\n",
    "print(\"Tokens:\", encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "adbbb061-db86-4254-9712-555da5a8aaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"my_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "c4cbaa3e-ca0b-4b6c-9a44-98635efe2f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['[CLS]', 'Hello', 'World', '!', 'My', 'name', 'is', 'Jay', ',', 'what', \"'\", 's', 'y', 'ours', '?', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "loaded_tokenizer = Tokenizer.from_file(\"my_tokenizer.json\")\n",
    "encoding = loaded_tokenizer.encode(\"Hello World! My name is Jay, what's yours?\")\n",
    "print(\"Tokens:\", encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "d2630eaf-b0b1-4a11-be60-09b514c31587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 80283, 1700, 5, 2899, 1730, 894, 6348, 16, 1729, 11, 87, 93, 2333, 35, 3]"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n",
    "wrapped_tokenizer.encode(\"Hello World! My name is Jay, what's yours?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e85479c-cc3c-4566-a61b-ccda001babb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
